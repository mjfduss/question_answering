services:

  llm: &llm
    image: ollama/ollama:latest
    profiles: [ "linux" ]
    networks:
      - net

  llm-gpu:
    <<: *llm
    profiles: [ "linux-gpu" ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

  pull-model:
    image: genai-stack/pull-model:latest
    build:
      context: .
      dockerfile: pull_model.Dockerfile
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://llm-gpu:11434}
      - LLM=${LLM-llama2}
    networks:
      - net
    tty: true

  database:
    image: neo4j:5.19
    ports:
      - 7687:7687
      - 7474:7474
    volumes:
      - $PWD/data:/data
    environment:
      - NEO4J_AUTH=${NEO4J_USERNAME-neo4j}/${NEO4J_PASSWORD-password}
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_db_tx__log_rotation_retention__policy=false
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 --spider localhost:7474 || exit 1"
        ]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - net

  coder:
    image: codercom/code-server:latest
    ports:
      - 8080:8080
    volumes:
      - $HOME/.config:/home/coder/.config
      - $PWD:/home/coder/project
    environment:
      - DOCKER_USER=$USER
    user: ${UID}:${GID}

  knowledgegraph:
    build:
      context: .
      dockerfile: knowledgegraph.Dockerfile
    volumes:
      - $PWD/embedding_model:/embedding_model
    environment:
      - NEO4J_URI=${NEO4J_URI-neo4j://database:7687}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD-password}
      - NEO4J_USERNAME=${NEO4J_USERNAME-neo4j}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://llm-gpu:11434}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL-sentence_transformer}
    networks:
      - net
    depends_on:
      database:
        condition: service_healthy
      pull-model:
        condition: service_completed_successfully
    x-develop:
      watch:
        - action: rebuild
          path: .
          ignore:
            - bot.py
    ports:
      - 8081:8080
      - 8502:8502

  bot:
    build:
      context: .
      dockerfile: bot.Dockerfile
    volumes:
      - $PWD/embedding_model:/embedding_model
    environment:
      - NEO4J_URI=${NEO4J_URI-neo4j://database:7687}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD-password}
      - NEO4J_USERNAME=${NEO4J_USERNAME-neo4j}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://llm-gpu:11434}
      - LLM=${LLM-llama2}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL-sentence_transformer}
    networks:
      - net
    depends_on:
      database:
        condition: service_healthy
      pull-model:
        condition: service_completed_successfully
    x-develop:
      watch:
        - action: rebuild
          path: .
          ignore:
            - knowledgegraph.py
            - pdf_bot.py
            - api.py
            - front-end/
    ports:
      - 8501:8501

networks:
  net:
